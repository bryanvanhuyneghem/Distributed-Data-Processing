{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data cleaning using Spark Streaming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This notebook reads data from the `ingest` topic on our Kafka distributed queue, cleans each of the messages and write the result to the Kafka topic `ingest-cleaned`.\n",
    "\n",
    "Spark Structured Streaming treats a live data stream as a table to which we can continuously append. We can run queries on this table to our heart's content. A new event results in a new record in the table, after which the result of the queries will be recomputed in an intelligent way so it does not have to recompute everything, but instead works with a delta change.  \n",
    "\n",
    "Spark is responsible for updating the results table when there is  new data and relieves us from maintaining running aggregrations, ensuring data consistency and fault tolerance. Everything is done for us, which makes our lives simpler, allowing us to focus on the essentials."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Input\n",
    "\n",
    "We will now use the Spark Structured Streaming API to clean our event stream. We will us e a DataStreamReader to read from a Kafka source. We have added events to our Kafka distributed queue in notebook `1_read_and_POST.ipynb` and are now ready to process them.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Output\n",
    "\n",
    "We will output our resulting data to a Kafka sink. Each row of our dataframe will be written to the Kafka topic `ingest-cleaned`. We will use the outpot mode `append`, which allows us to append new rows to the results table."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Cleaning the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: kafka-python in /opt/conda/lib/python3.8/site-packages (2.0.2)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Ensure the required Python 3 dependencies are installed.\n",
    "python3 -m pip install kafka-python"
   ]
  },
  {
   "source": [
    "We will now create a Spark context and specify that the Python spark-kafka libraries need to be added."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "import pyspark \n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a local Spark cluster with two executors (if it doesn't already exist)\n",
    "spark = SparkSession.builder.master('local[2]').getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "source": [
    "We will now creating a streaming DataFrame that respresents the events received from the Kafka topic `ingest`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\",\"localhost:9092\")\n",
    "    # Change `ingest` to your topic of choice.\n",
    "    .option(\"subscribe\", \"ingest-test\")\n",
    "    # earliest: start reading from the beginning of the queue\n",
    "    # this will also read all messages already present on the Kafka topic\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n"
   ]
  },
  {
   "source": [
    "We can't just run the query and see the output because the query will never stop. After all, these are streaming dataframes. For debugging purposes, we can spin up a query, wait a few seconds so we have some results, and show the contents of the in-memory table.\n",
    "\n",
    "We stop the running query so we don't run out of memory."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_decoded = (\n",
    "    input\n",
    "    .withColumn(\"value\", input[\"value\"].cast(\"string\"))\n",
    "    .select(\"value\", \"timestamp\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "{'message': 'Waiting for data to arrive',\n 'isDataAvailable': False,\n 'isTriggerActive': False}"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                                  value  \\\n0     {\"lat\": 40.297875899999994, \"lng\": -75.5812935...   \n1     {\"lat\": 40.2580614, \"lng\": -75.26467990000002,...   \n2     {\"lat\": 40.121181799999995, \"lng\": -75.3519752...   \n3     {\"lat\": 40.116153000000004, \"lng\": -75.343513,...   \n4     {\"lat\": 40.251492, \"lng\": -75.6033497, \"desc\":...   \n...                                                 ...   \n9995  {\"lat\": 40.075536299999996, \"lng\": -75.3046354...   \n9996  {\"lat\": 40.2116628, \"lng\": -75.2759685, \"desc\"...   \n9997  {\"lat\": 40.069013, \"lng\": -75.134458, \"desc\": ...   \n9998  {\"lat\": 40.3126186, \"lng\": -75.31258270000001,...   \n9999  {\"lat\": 40.3714526, \"lng\": -75.48407579999999,...   \n\n                   timestamp  \n0    2020-12-03 15:35:48.143  \n1    2020-12-03 15:35:48.157  \n2    2020-12-03 15:35:48.171  \n3    2020-12-03 15:35:48.186  \n4    2020-12-03 15:35:48.204  \n...                      ...  \n9995 2020-12-03 15:38:07.331  \n9996 2020-12-03 15:38:07.344  \n9997 2020-12-03 15:38:07.360  \n9998 2020-12-03 15:38:07.374  \n9999 2020-12-03 15:38:07.391  \n\n[10000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>value</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>{\"lat\": 40.297875899999994, \"lng\": -75.5812935...</td>\n      <td>2020-12-03 15:35:48.143</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>{\"lat\": 40.2580614, \"lng\": -75.26467990000002,...</td>\n      <td>2020-12-03 15:35:48.157</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>{\"lat\": 40.121181799999995, \"lng\": -75.3519752...</td>\n      <td>2020-12-03 15:35:48.171</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>{\"lat\": 40.116153000000004, \"lng\": -75.343513,...</td>\n      <td>2020-12-03 15:35:48.186</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>{\"lat\": 40.251492, \"lng\": -75.6033497, \"desc\":...</td>\n      <td>2020-12-03 15:35:48.204</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>{\"lat\": 40.075536299999996, \"lng\": -75.3046354...</td>\n      <td>2020-12-03 15:38:07.331</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>{\"lat\": 40.2116628, \"lng\": -75.2759685, \"desc\"...</td>\n      <td>2020-12-03 15:38:07.344</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>{\"lat\": 40.069013, \"lng\": -75.134458, \"desc\": ...</td>\n      <td>2020-12-03 15:38:07.360</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>{\"lat\": 40.3126186, \"lng\": -75.31258270000001,...</td>\n      <td>2020-12-03 15:38:07.374</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>{\"lat\": 40.3714526, \"lng\": -75.48407579999999,...</td>\n      <td>2020-12-03 15:38:07.391</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "try:\n",
    "    # In case the previous query wasn't stopped\n",
    "    tq.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tq = (\n",
    "    # Create an output stream\n",
    "    stream_decoded.writeStream               \n",
    "    # Only write new rows to the output\n",
    "    # To clean data, we can only use the outputMode 'append'\n",
    "    .outputMode(\"append\")           \n",
    "    # Write output stream to an in-memory Spark table (a DataFrame)\n",
    "    .format(\"memory\")               \n",
    "    # The name of the output table will be the same as the name of the query\n",
    "    .queryName(\"test_query\")\n",
    "    # Submit the query to Spark and execute it\n",
    "    .start()\n",
    ")\n",
    "\n",
    "sleep(2)\n",
    "\n",
    "# When the status says \"Waiting for data to arrive\", that means the query\n",
    "# has finished its current iteration and is waiting for new messages from\n",
    "# Kafka.\n",
    "display(tq.status)\n",
    "\n",
    "memory_sink = spark.table(\"test_query\")\n",
    "# Show result table in Jupyter Notebook. Since Jupyter Notebooks have native support for showing pandas tables,\n",
    "# we convert the Spark DataFrame.\n",
    "display(memory_sink.toPandas())\n",
    "\n",
    "# Stop the query\n",
    "tq.stop()"
   ]
  },
  {
   "source": [
    "We use the `from_json` function to convert our JSON to a tuple in one column. We will later flatten this column so that each field of our tuple becomes a column in our DataFrame. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lat,lng,desc,zip,title,timeStamp,twp,addr,e\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"lat\", DoubleType()),\n",
    "    StructField(\"lng\", DoubleType()),\n",
    "    StructField(\"desc\", StringType()),\n",
    "    StructField(\"zip\", FloatType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"timeStamp\", TimestampType()),\n",
    "    StructField(\"twp\", StringType()),\n",
    "    StructField(\"addr\", StringType()),\n",
    "    StructField(\"e\", IntegerType()),\n",
    "])\n",
    "\n",
    "decoded_json_stream = (\n",
    "    stream_decoded.withColumn(\"nineoneone\", from_json(col(\"value\"), schema))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "flattened_stream = (\n",
    "    decoded_json_stream\n",
    "    .select(\"nineoneone.*\") \n",
    ")\n",
    "# Create two requested columns from column 'title'\n",
    "split_col = pyspark.sql.functions.split(flattened_stream['title'], ':')\n",
    "flattened_stream = flattened_stream.withColumn('majorTitle', split_col.getItem(0)).withColumn('minorTitle', split_col.getItem(1))\n",
    "\n",
    "# Deal with NaN\n",
    "flattened_stream = flattened_stream.replace(float('nan'), None)\n",
    "flattened_stream = flattened_stream.withColumn(\"zip\", col(\"zip\").cast(IntegerType()))\n",
    "\n",
    "# Make columns hour, date\n",
    "flattened_stream = flattened_stream.withColumn(\"hour\", hour(col(\"timeStamp\")).cast(IntegerType()))\n",
    "flattened_stream = flattened_stream.withColumn(\"date\", to_date(col(\"timeStamp\")))"
   ]
  },
  {
   "source": [
    "Let's take a look at our flattened stream. We will do so using Pandas and we can also see that our columns are typed appropriately by using `dtypes`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "{'message': 'Getting offsets from KafkaV2[Subscribe[ingest-test]]',\n 'isDataAvailable': False,\n 'isTriggerActive': True}"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "         lat        lng                                               desc  \\\n0  40.297876 -75.581294  REINDEER CT & DEAD END;  NEW HANOVER; Station ...   \n1  40.258061 -75.264680  BRIAR PATH & WHITEMARSH LN;  HATFIELD TOWNSHIP...   \n2  40.121182 -75.351975  HAWS AVE; NORRISTOWN; 2015-12-10 @ 14:39:21-St...   \n3  40.116153 -75.343513  AIRY ST & SWEDE ST;  NORRISTOWN; Station 308A;...   \n4  40.251492 -75.603350  CHERRYWOOD CT & DEAD END;  LOWER POTTSGROVE; S...   \n5  40.253473 -75.283245  CANNON AVE & W 9TH ST;  LANSDALE; Station 345;...   \n6  40.182111 -75.127795  LAUREL AVE & OAKDALE AVE;  HORSHAM; Station 35...   \n7  40.217286 -75.405182  COLLEGEVILLE RD & LYWISKI RD;  SKIPPACK; Stati...   \n8  40.289027 -75.399590  MAIN ST & OLD SUMNEYTOWN PIKE;  LOWER SALFORD;...   \n9  40.102398 -75.291458  BLUEROUTE  & RAMP I476 NB TO CHEMICAL RD; PLYM...   \n\n       zip                        title           timeStamp  \\\n0  19525.0       EMS: BACK PAINS/INJURY 2015-12-10 17:10:52   \n1  19446.0      EMS: DIABETIC EMERGENCY 2015-12-10 17:29:21   \n2  19401.0          Fire: GAS-ODOR/LEAK 2015-12-10 14:39:21   \n3  19401.0       EMS: CARDIAC EMERGENCY 2015-12-10 16:47:36   \n4      NaN               EMS: DIZZINESS 2015-12-10 16:56:52   \n5  19446.0             EMS: HEAD INJURY 2015-12-10 15:39:04   \n6  19044.0         EMS: NAUSEA/VOMITING 2015-12-10 16:46:48   \n7  19426.0   EMS: RESPIRATORY EMERGENCY 2015-12-10 16:17:05   \n8  19438.0        EMS: SYNCOPAL EPISODE 2015-12-10 16:51:42   \n9  19462.0  Traffic: VEHICLE ACCIDENT - 2015-12-10 17:35:41   \n\n                 twp                                      addr  e majorTitle  \\\n0        NEW HANOVER                    REINDEER CT & DEAD END  1        EMS   \n1  HATFIELD TOWNSHIP                BRIAR PATH & WHITEMARSH LN  1        EMS   \n2         NORRISTOWN                                  HAWS AVE  1       Fire   \n3         NORRISTOWN                        AIRY ST & SWEDE ST  1        EMS   \n4   LOWER POTTSGROVE                  CHERRYWOOD CT & DEAD END  1        EMS   \n5           LANSDALE                     CANNON AVE & W 9TH ST  1        EMS   \n6            HORSHAM                  LAUREL AVE & OAKDALE AVE  1        EMS   \n7           SKIPPACK              COLLEGEVILLE RD & LYWISKI RD  1        EMS   \n8      LOWER SALFORD             MAIN ST & OLD SUMNEYTOWN PIKE  1        EMS   \n9           PLYMOUTH  BLUEROUTE  & RAMP I476 NB TO CHEMICAL RD  1    Traffic   \n\n               minorTitle  hour        date  \n0       BACK PAINS/INJURY    17  2015-12-10  \n1      DIABETIC EMERGENCY    17  2015-12-10  \n2           GAS-ODOR/LEAK    14  2015-12-10  \n3       CARDIAC EMERGENCY    16  2015-12-10  \n4               DIZZINESS    16  2015-12-10  \n5             HEAD INJURY    15  2015-12-10  \n6         NAUSEA/VOMITING    16  2015-12-10  \n7   RESPIRATORY EMERGENCY    16  2015-12-10  \n8        SYNCOPAL EPISODE    16  2015-12-10  \n9      VEHICLE ACCIDENT -    17  2015-12-10  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lat</th>\n      <th>lng</th>\n      <th>desc</th>\n      <th>zip</th>\n      <th>title</th>\n      <th>timeStamp</th>\n      <th>twp</th>\n      <th>addr</th>\n      <th>e</th>\n      <th>majorTitle</th>\n      <th>minorTitle</th>\n      <th>hour</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>40.297876</td>\n      <td>-75.581294</td>\n      <td>REINDEER CT &amp; DEAD END;  NEW HANOVER; Station ...</td>\n      <td>19525.0</td>\n      <td>EMS: BACK PAINS/INJURY</td>\n      <td>2015-12-10 17:10:52</td>\n      <td>NEW HANOVER</td>\n      <td>REINDEER CT &amp; DEAD END</td>\n      <td>1</td>\n      <td>EMS</td>\n      <td>BACK PAINS/INJURY</td>\n      <td>17</td>\n      <td>2015-12-10</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>40.258061</td>\n      <td>-75.264680</td>\n      <td>BRIAR PATH &amp; WHITEMARSH LN;  HATFIELD TOWNSHIP...</td>\n      <td>19446.0</td>\n      <td>EMS: DIABETIC EMERGENCY</td>\n      <td>2015-12-10 17:29:21</td>\n      <td>HATFIELD TOWNSHIP</td>\n      <td>BRIAR PATH &amp; WHITEMARSH LN</td>\n      <td>1</td>\n      <td>EMS</td>\n      <td>DIABETIC EMERGENCY</td>\n      <td>17</td>\n      <td>2015-12-10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>40.121182</td>\n      <td>-75.351975</td>\n      <td>HAWS AVE; NORRISTOWN; 2015-12-10 @ 14:39:21-St...</td>\n      <td>19401.0</td>\n      <td>Fire: GAS-ODOR/LEAK</td>\n      <td>2015-12-10 14:39:21</td>\n      <td>NORRISTOWN</td>\n      <td>HAWS AVE</td>\n      <td>1</td>\n      <td>Fire</td>\n      <td>GAS-ODOR/LEAK</td>\n      <td>14</td>\n      <td>2015-12-10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>40.116153</td>\n      <td>-75.343513</td>\n      <td>AIRY ST &amp; SWEDE ST;  NORRISTOWN; Station 308A;...</td>\n      <td>19401.0</td>\n      <td>EMS: CARDIAC EMERGENCY</td>\n      <td>2015-12-10 16:47:36</td>\n      <td>NORRISTOWN</td>\n      <td>AIRY ST &amp; SWEDE ST</td>\n      <td>1</td>\n      <td>EMS</td>\n      <td>CARDIAC EMERGENCY</td>\n      <td>16</td>\n      <td>2015-12-10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>40.251492</td>\n      <td>-75.603350</td>\n      <td>CHERRYWOOD CT &amp; DEAD END;  LOWER POTTSGROVE; S...</td>\n      <td>NaN</td>\n      <td>EMS: DIZZINESS</td>\n      <td>2015-12-10 16:56:52</td>\n      <td>LOWER POTTSGROVE</td>\n      <td>CHERRYWOOD CT &amp; DEAD END</td>\n      <td>1</td>\n      <td>EMS</td>\n      <td>DIZZINESS</td>\n      <td>16</td>\n      <td>2015-12-10</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>40.253473</td>\n      <td>-75.283245</td>\n      <td>CANNON AVE &amp; W 9TH ST;  LANSDALE; Station 345;...</td>\n      <td>19446.0</td>\n      <td>EMS: HEAD INJURY</td>\n      <td>2015-12-10 15:39:04</td>\n      <td>LANSDALE</td>\n      <td>CANNON AVE &amp; W 9TH ST</td>\n      <td>1</td>\n      <td>EMS</td>\n      <td>HEAD INJURY</td>\n      <td>15</td>\n      <td>2015-12-10</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>40.182111</td>\n      <td>-75.127795</td>\n      <td>LAUREL AVE &amp; OAKDALE AVE;  HORSHAM; Station 35...</td>\n      <td>19044.0</td>\n      <td>EMS: NAUSEA/VOMITING</td>\n      <td>2015-12-10 16:46:48</td>\n      <td>HORSHAM</td>\n      <td>LAUREL AVE &amp; OAKDALE AVE</td>\n      <td>1</td>\n      <td>EMS</td>\n      <td>NAUSEA/VOMITING</td>\n      <td>16</td>\n      <td>2015-12-10</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>40.217286</td>\n      <td>-75.405182</td>\n      <td>COLLEGEVILLE RD &amp; LYWISKI RD;  SKIPPACK; Stati...</td>\n      <td>19426.0</td>\n      <td>EMS: RESPIRATORY EMERGENCY</td>\n      <td>2015-12-10 16:17:05</td>\n      <td>SKIPPACK</td>\n      <td>COLLEGEVILLE RD &amp; LYWISKI RD</td>\n      <td>1</td>\n      <td>EMS</td>\n      <td>RESPIRATORY EMERGENCY</td>\n      <td>16</td>\n      <td>2015-12-10</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>40.289027</td>\n      <td>-75.399590</td>\n      <td>MAIN ST &amp; OLD SUMNEYTOWN PIKE;  LOWER SALFORD;...</td>\n      <td>19438.0</td>\n      <td>EMS: SYNCOPAL EPISODE</td>\n      <td>2015-12-10 16:51:42</td>\n      <td>LOWER SALFORD</td>\n      <td>MAIN ST &amp; OLD SUMNEYTOWN PIKE</td>\n      <td>1</td>\n      <td>EMS</td>\n      <td>SYNCOPAL EPISODE</td>\n      <td>16</td>\n      <td>2015-12-10</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>40.102398</td>\n      <td>-75.291458</td>\n      <td>BLUEROUTE  &amp; RAMP I476 NB TO CHEMICAL RD; PLYM...</td>\n      <td>19462.0</td>\n      <td>Traffic: VEHICLE ACCIDENT -</td>\n      <td>2015-12-10 17:35:41</td>\n      <td>PLYMOUTH</td>\n      <td>BLUEROUTE  &amp; RAMP I476 NB TO CHEMICAL RD</td>\n      <td>1</td>\n      <td>Traffic</td>\n      <td>VEHICLE ACCIDENT -</td>\n      <td>17</td>\n      <td>2015-12-10</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "[('lat', 'double'),\n ('lng', 'double'),\n ('desc', 'string'),\n ('zip', 'int'),\n ('title', 'string'),\n ('timeStamp', 'timestamp'),\n ('twp', 'string'),\n ('addr', 'string'),\n ('e', 'int'),\n ('majorTitle', 'string'),\n ('minorTitle', 'string'),\n ('hour', 'int'),\n ('date', 'date')]"
     },
     "metadata": {}
    }
   ],
   "source": [
    "try:\n",
    "    # In case the previous query wasn't stopped\n",
    "    tq.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tq = (\n",
    "    # Create an output stream\n",
    "    flattened_stream.writeStream               \n",
    "    # Only write new rows to the output\n",
    "    .outputMode(\"append\")           \n",
    "    # Write output stream to an in-memory Spark table (a DataFrame)\n",
    "    .format(\"memory\")               \n",
    "    # The name of the output table will be the same as the name of the query\n",
    "    .queryName(\"test_query\")\n",
    "    # Submit the query to Spark and execute it\n",
    "    .start()\n",
    ")\n",
    "\n",
    "sleep(2)\n",
    "\n",
    "# When the status says \"Waiting for data to arrive\", that means the query\n",
    "# has finished its current iteration and is waiting for new messages from\n",
    "# Kafka.\n",
    "display(tq.status)\n",
    "\n",
    "memory_sink = spark.table(\"test_query\")\n",
    "\n",
    "\n",
    "# Show result table in Jupyter Notebook. Since Jupyter Notebooks have native support for showing pandas tables,\n",
    "# we convert the Spark DataFrame.\n",
    "display(memory_sink.toPandas().head(10))\n",
    "display(memory_sink.dtypes)\n",
    "\n",
    "# Stop the query\n",
    "tq.stop()"
   ]
  },
  {
   "source": [
    "## Write entries to ingest-cleaned Kafka topic"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Finally, we want to write the cleaned 911 entries to the `ingest-cleaned` Kafka topic. This Kafka output stream expects a dataframe, a value and an optional key column.\n",
    "\n",
    "To create the `value` column, we first create a struct from all columns in the dataframe by using the `struct` function, serialize the result to json using `to_json`, and keep only the value column using `select` and `alias`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_stream = flattened_stream.select(to_json(struct(\"*\")).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "{'message': 'Processing new data',\n 'isDataAvailable': True,\n 'isTriggerActive': True}"
     },
     "metadata": {}
    }
   ],
   "source": [
    "try:\n",
    "    # In case the previous query wasn't stopped\n",
    "    tq.stop()\n",
    "    # Remove old checkpoint dir, otherwise you'll get weird runtime faults\n",
    "    os.rmdir(\"checkpoints-cleanup\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Prepare df for Kafka and write to kafka\n",
    "tq = (\n",
    "    output_stream\n",
    "    .writeStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"topic\", \"ingest-cleaned\")\n",
    "    .option(\"checkpointLocation\", \"checkpoints-cleanup\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "sleep(2)\n",
    "display(tq.status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}