{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data cleaning using Spark Streaming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This notebook reads data from the `ingest` topic on our Kafka distributed queue, cleans each of the messages and write the result to the Kafka topic `ingest-cleaned`.\n",
    "\n",
    "Spark Structured Streaming treats a live data stream as a table to which we can continuously append. We can run queries on this table to our heart's content. A new event results in a new record in the table, after which the result of the queries will be recomputed in an intelligent way so it does not have to recompute everything, but instead works with a delta change.  \n",
    "\n",
    "Spark is responsible for updating the results table when there is  new data and relieves us from maintaining running aggregrations, ensuring data consistency and fault tolerance. Everything is done for us, which makes our lives simpler, allowing us to focus on the essentials."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Input\n",
    "\n",
    "We will now use the Spark Structured Streaming API to clean our event stream. We will us e a DataStreamReader to read from a Kafka source. We have added events to our Kafka distributed queue in notebook `1_read_and_POST.ipynb` and are now ready to process them.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Output\n",
    "\n",
    "We will output our resulting data to a Kafka sink. Each row of our dataframe will be written to the Kafka topic `ingest-cleaned`. We will use the outpot mode `append`, which allows us to append new rows to the results table."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Cleaning the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: kafka-python in /opt/conda/lib/python3.8/site-packages (2.0.2)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Ensure the required Python 3 dependencies are installed.\n",
    "python3 -m pip install kafka-python"
   ]
  },
  {
   "source": [
    "We will now create a Spark context and specify that the Python spark-kafka libraries need to be added."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "import pyspark \n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a local Spark cluster with two executors (if it doesn't already exist)\n",
    "spark = SparkSession.builder.master('local[2]').getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "source": [
    "We will now creating a streaming DataFrame that respresents the events received from the Kafka topic `ingest`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\",\"localhost:9092\")\n",
    "    # Change `ingest` to your topic of choice.\n",
    "    .option(\"subscribe\", \"ingest\")\n",
    "    # earliest: start reading from the beginning of the queue\n",
    "    # this will also read all messages already present on the Kafka topic\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n"
   ]
  },
  {
   "source": [
    "We can't just run the query and see the output because the query will never stop. After all, these are streaming dataframes. For debugging purposes, we can spin up a query, wait a few seconds so we have some results, and show the contents of the in-memory table.\n",
    "\n",
    "We stop the running query so we don't run out of memory."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_decoded = (\n",
    "    input\n",
    "    .withColumn(\"value\", input[\"value\"].cast(\"string\"))\n",
    "    .select(\"value\", \"timestamp\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "{'message': 'Waiting for data to arrive',\n 'isDataAvailable': False,\n 'isTriggerActive': False}"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                                  value  \\\n0     {\"lat\": 40.297875899999994, \"lng\": -75.5812935...   \n1     {\"lat\": 40.2580614, \"lng\": -75.26467990000002,...   \n2     {\"lat\": 40.121181799999995, \"lng\": -75.3519752...   \n3     {\"lat\": 40.116153000000004, \"lng\": -75.343513,...   \n4     {\"lat\": 40.251492, \"lng\": -75.6033497, \"desc\":...   \n...                                                 ...   \n5777  {\"lat\": 40.114239000000005, \"lng\": -75.3385079...   \n5778  {\"lat\": 40.1179476, \"lng\": -75.20984759999999,...   \n5779  {\"lat\": 40.1990064, \"lng\": -75.3000584, \"desc\"...   \n5780  {\"lat\": 40.143325700000005, \"lng\": -75.4228189...   \n5781  {\"lat\": 40.1532684, \"lng\": -75.18955759999999,...   \n\n                   timestamp  \n0    2020-11-26 11:11:49.990  \n1    2020-11-26 11:11:50.359  \n2    2020-11-26 11:11:51.034  \n3    2020-11-26 11:11:51.659  \n4    2020-11-26 11:11:52.623  \n...                      ...  \n5777 2020-11-26 21:26:02.046  \n5778 2020-11-26 21:26:02.175  \n5779 2020-11-26 21:26:02.226  \n5780 2020-11-26 21:26:02.295  \n5781 2020-11-26 21:26:02.404  \n\n[5782 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>value</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>{\"lat\": 40.297875899999994, \"lng\": -75.5812935...</td>\n      <td>2020-11-26 11:11:49.990</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>{\"lat\": 40.2580614, \"lng\": -75.26467990000002,...</td>\n      <td>2020-11-26 11:11:50.359</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>{\"lat\": 40.121181799999995, \"lng\": -75.3519752...</td>\n      <td>2020-11-26 11:11:51.034</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>{\"lat\": 40.116153000000004, \"lng\": -75.343513,...</td>\n      <td>2020-11-26 11:11:51.659</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>{\"lat\": 40.251492, \"lng\": -75.6033497, \"desc\":...</td>\n      <td>2020-11-26 11:11:52.623</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5777</th>\n      <td>{\"lat\": 40.114239000000005, \"lng\": -75.3385079...</td>\n      <td>2020-11-26 21:26:02.046</td>\n    </tr>\n    <tr>\n      <th>5778</th>\n      <td>{\"lat\": 40.1179476, \"lng\": -75.20984759999999,...</td>\n      <td>2020-11-26 21:26:02.175</td>\n    </tr>\n    <tr>\n      <th>5779</th>\n      <td>{\"lat\": 40.1990064, \"lng\": -75.3000584, \"desc\"...</td>\n      <td>2020-11-26 21:26:02.226</td>\n    </tr>\n    <tr>\n      <th>5780</th>\n      <td>{\"lat\": 40.143325700000005, \"lng\": -75.4228189...</td>\n      <td>2020-11-26 21:26:02.295</td>\n    </tr>\n    <tr>\n      <th>5781</th>\n      <td>{\"lat\": 40.1532684, \"lng\": -75.18955759999999,...</td>\n      <td>2020-11-26 21:26:02.404</td>\n    </tr>\n  </tbody>\n</table>\n<p>5782 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "try:\n",
    "    # In case the previous query wasn't stopped\n",
    "    tq.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tq = (\n",
    "    # Create an output stream\n",
    "    stream_decoded.writeStream               \n",
    "    # Only write new rows to the output\n",
    "    # To clean data, we can only use the outputMode 'append'\n",
    "    .outputMode(\"append\")           \n",
    "    # Write output stream to an in-memory Spark table (a DataFrame)\n",
    "    .format(\"memory\")               \n",
    "    # The name of the output table will be the same as the name of the query\n",
    "    .queryName(\"test_query\")\n",
    "    # Submit the query to Spark and execute it\n",
    "    .start()\n",
    ")\n",
    "\n",
    "sleep(2)\n",
    "\n",
    "# When the status says \"Waiting for data to arrive\", that means the query\n",
    "# has finished its current iteration and is waiting for new messages from\n",
    "# Kafka.\n",
    "display(tq.status)\n",
    "\n",
    "memory_sink = spark.table(\"test_query\")\n",
    "# Show result table in Jupyter Notebook. Since Jupyter Notebooks have native support for showing pandas tables,\n",
    "# we convert the Spark DataFrame.\n",
    "display(memory_sink.toPandas())\n",
    "\n",
    "# Stop the query\n",
    "tq.stop()"
   ]
  },
  {
   "source": [
    "We use the `from_json` function to convert our JSON to a tuple in one column. We will later flatten this column so that each field of our tuple becomes a column in our DataFrame. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lat,lng,desc,zip,title,timeStamp,twp,addr,e\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"lat\", DoubleType()),\n",
    "    StructField(\"lng\", DoubleType()),\n",
    "    StructField(\"desc\", StringType()),\n",
    "    StructField(\"zip\", FloatType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"timeStamp\", TimestampType()),\n",
    "    StructField(\"twp\", StringType()),\n",
    "    StructField(\"addr\", StringType()),\n",
    "    StructField(\"e\", IntegerType()),\n",
    "])\n",
    "\n",
    "decoded_json_stream = (\n",
    "    stream_decoded.withColumn(\"nineoneone\", from_json(col(\"value\"), schema))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "flattened_stream = (\n",
    "    decoded_json_stream\n",
    "    .select(\"nineoneone.*\") \n",
    ")\n",
    "# Create two requested columns from column 'title'\n",
    "split_col = pyspark.sql.functions.split(flattened_stream['title'], ':')\n",
    "flattened_stream = flattened_stream.withColumn('majorTitle', split_col.getItem(0)).withColumn('minorTitle', split_col.getItem(1))\n",
    "\n",
    "# Deal with NaN\n",
    "flattened_stream = flattened_stream.replace(float('nan'), None)\n",
    "flattened_stream = flattened_stream.withColumn(\"zip\", flattened_stream[\"zip\"].cast(IntegerType()))\n",
    "\n"
   ]
  },
  {
   "source": [
    "Let's take a look at our flattened stream. We will do so using Pandas and we can also see that our columns are typed appropriately by using `dtypes`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "{'message': 'Waiting for data to arrive',\n 'isDataAvailable': False,\n 'isTriggerActive': False}"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "         lat        lng                                               desc  \\\n0  40.297876 -75.581294  REINDEER CT & DEAD END;  NEW HANOVER; Station ...   \n1  40.258061 -75.264680  BRIAR PATH & WHITEMARSH LN;  HATFIELD TOWNSHIP...   \n2  40.121182 -75.351975  HAWS AVE; NORRISTOWN; 2015-12-10 @ 14:39:21-St...   \n3  40.116153 -75.343513  AIRY ST & SWEDE ST;  NORRISTOWN; Station 308A;...   \n4  40.251492 -75.603350  CHERRYWOOD CT & DEAD END;  LOWER POTTSGROVE; S...   \n5  40.253473 -75.283245  CANNON AVE & W 9TH ST;  LANSDALE; Station 345;...   \n6  40.182111 -75.127795  LAUREL AVE & OAKDALE AVE;  HORSHAM; Station 35...   \n7  40.217286 -75.405182  COLLEGEVILLE RD & LYWISKI RD;  SKIPPACK; Stati...   \n8  40.289027 -75.399590  MAIN ST & OLD SUMNEYTOWN PIKE;  LOWER SALFORD;...   \n9  40.102398 -75.291458  BLUEROUTE  & RAMP I476 NB TO CHEMICAL RD; PLYM...   \n\n       zip                        title           timeStamp  \\\n0  19525.0       EMS: BACK PAINS/INJURY 2015-12-10 17:10:52   \n1  19446.0      EMS: DIABETIC EMERGENCY 2015-12-10 17:29:21   \n2  19401.0          Fire: GAS-ODOR/LEAK 2015-12-10 14:39:21   \n3  19401.0       EMS: CARDIAC EMERGENCY 2015-12-10 16:47:36   \n4      NaN               EMS: DIZZINESS 2015-12-10 16:56:52   \n5  19446.0             EMS: HEAD INJURY 2015-12-10 15:39:04   \n6  19044.0         EMS: NAUSEA/VOMITING 2015-12-10 16:46:48   \n7  19426.0   EMS: RESPIRATORY EMERGENCY 2015-12-10 16:17:05   \n8  19438.0        EMS: SYNCOPAL EPISODE 2015-12-10 16:51:42   \n9  19462.0  Traffic: VEHICLE ACCIDENT - 2015-12-10 17:35:41   \n\n                 twp                                      addr  e majorTitle  \\\n0        NEW HANOVER                    REINDEER CT & DEAD END  1        EMS   \n1  HATFIELD TOWNSHIP                BRIAR PATH & WHITEMARSH LN  1        EMS   \n2         NORRISTOWN                                  HAWS AVE  1       Fire   \n3         NORRISTOWN                        AIRY ST & SWEDE ST  1        EMS   \n4   LOWER POTTSGROVE                  CHERRYWOOD CT & DEAD END  1        EMS   \n5           LANSDALE                     CANNON AVE & W 9TH ST  1        EMS   \n6            HORSHAM                  LAUREL AVE & OAKDALE AVE  1        EMS   \n7           SKIPPACK              COLLEGEVILLE RD & LYWISKI RD  1        EMS   \n8      LOWER SALFORD             MAIN ST & OLD SUMNEYTOWN PIKE  1        EMS   \n9           PLYMOUTH  BLUEROUTE  & RAMP I476 NB TO CHEMICAL RD  1    Traffic   \n\n               minorTitle  \n0       BACK PAINS/INJURY  \n1      DIABETIC EMERGENCY  \n2           GAS-ODOR/LEAK  \n3       CARDIAC EMERGENCY  \n4               DIZZINESS  \n5             HEAD INJURY  \n6         NAUSEA/VOMITING  \n7   RESPIRATORY EMERGENCY  \n8        SYNCOPAL EPISODE  \n9      VEHICLE ACCIDENT -  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lat</th>\n      <th>lng</th>\n      <th>desc</th>\n      <th>zip</th>\n      <th>title</th>\n      <th>timeStamp</th>\n      <th>twp</th>\n      <th>addr</th>\n      <th>e</th>\n      <th>majorTitle</th>\n      <th>minorTitle</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>40.297876</td>\n      <td>-75.581294</td>\n      <td>REINDEER CT &amp; DEAD END;  NEW HANOVER; Station ...</td>\n      <td>19525.0</td>\n      <td>EMS: BACK PAINS/INJURY</td>\n      <td>2015-12-10 17:10:52</td>\n      <td>NEW HANOVER</td>\n      <td>REINDEER CT &amp; DEAD END</td>\n      <td>1</td>\n      <td>EMS</td>\n      <td>BACK PAINS/INJURY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>40.258061</td>\n      <td>-75.264680</td>\n      <td>BRIAR PATH &amp; WHITEMARSH LN;  HATFIELD TOWNSHIP...</td>\n      <td>19446.0</td>\n      <td>EMS: DIABETIC EMERGENCY</td>\n      <td>2015-12-10 17:29:21</td>\n      <td>HATFIELD TOWNSHIP</td>\n      <td>BRIAR PATH &amp; WHITEMARSH LN</td>\n      <td>1</td>\n      <td>EMS</td>\n      <td>DIABETIC EMERGENCY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>40.121182</td>\n      <td>-75.351975</td>\n      <td>HAWS AVE; NORRISTOWN; 2015-12-10 @ 14:39:21-St...</td>\n      <td>19401.0</td>\n      <td>Fire: GAS-ODOR/LEAK</td>\n      <td>2015-12-10 14:39:21</td>\n      <td>NORRISTOWN</td>\n      <td>HAWS AVE</td>\n      <td>1</td>\n      <td>Fire</td>\n      <td>GAS-ODOR/LEAK</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>40.116153</td>\n      <td>-75.343513</td>\n      <td>AIRY ST &amp; SWEDE ST;  NORRISTOWN; Station 308A;...</td>\n      <td>19401.0</td>\n      <td>EMS: CARDIAC EMERGENCY</td>\n      <td>2015-12-10 16:47:36</td>\n      <td>NORRISTOWN</td>\n      <td>AIRY ST &amp; SWEDE ST</td>\n      <td>1</td>\n      <td>EMS</td>\n      <td>CARDIAC EMERGENCY</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>40.251492</td>\n      <td>-75.603350</td>\n      <td>CHERRYWOOD CT &amp; DEAD END;  LOWER POTTSGROVE; S...</td>\n      <td>NaN</td>\n      <td>EMS: DIZZINESS</td>\n      <td>2015-12-10 16:56:52</td>\n      <td>LOWER POTTSGROVE</td>\n      <td>CHERRYWOOD CT &amp; DEAD END</td>\n      <td>1</td>\n      <td>EMS</td>\n      <td>DIZZINESS</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>40.253473</td>\n      <td>-75.283245</td>\n      <td>CANNON AVE &amp; W 9TH ST;  LANSDALE; Station 345;...</td>\n      <td>19446.0</td>\n      <td>EMS: HEAD INJURY</td>\n      <td>2015-12-10 15:39:04</td>\n      <td>LANSDALE</td>\n      <td>CANNON AVE &amp; W 9TH ST</td>\n      <td>1</td>\n      <td>EMS</td>\n      <td>HEAD INJURY</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>40.182111</td>\n      <td>-75.127795</td>\n      <td>LAUREL AVE &amp; OAKDALE AVE;  HORSHAM; Station 35...</td>\n      <td>19044.0</td>\n      <td>EMS: NAUSEA/VOMITING</td>\n      <td>2015-12-10 16:46:48</td>\n      <td>HORSHAM</td>\n      <td>LAUREL AVE &amp; OAKDALE AVE</td>\n      <td>1</td>\n      <td>EMS</td>\n      <td>NAUSEA/VOMITING</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>40.217286</td>\n      <td>-75.405182</td>\n      <td>COLLEGEVILLE RD &amp; LYWISKI RD;  SKIPPACK; Stati...</td>\n      <td>19426.0</td>\n      <td>EMS: RESPIRATORY EMERGENCY</td>\n      <td>2015-12-10 16:17:05</td>\n      <td>SKIPPACK</td>\n      <td>COLLEGEVILLE RD &amp; LYWISKI RD</td>\n      <td>1</td>\n      <td>EMS</td>\n      <td>RESPIRATORY EMERGENCY</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>40.289027</td>\n      <td>-75.399590</td>\n      <td>MAIN ST &amp; OLD SUMNEYTOWN PIKE;  LOWER SALFORD;...</td>\n      <td>19438.0</td>\n      <td>EMS: SYNCOPAL EPISODE</td>\n      <td>2015-12-10 16:51:42</td>\n      <td>LOWER SALFORD</td>\n      <td>MAIN ST &amp; OLD SUMNEYTOWN PIKE</td>\n      <td>1</td>\n      <td>EMS</td>\n      <td>SYNCOPAL EPISODE</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>40.102398</td>\n      <td>-75.291458</td>\n      <td>BLUEROUTE  &amp; RAMP I476 NB TO CHEMICAL RD; PLYM...</td>\n      <td>19462.0</td>\n      <td>Traffic: VEHICLE ACCIDENT -</td>\n      <td>2015-12-10 17:35:41</td>\n      <td>PLYMOUTH</td>\n      <td>BLUEROUTE  &amp; RAMP I476 NB TO CHEMICAL RD</td>\n      <td>1</td>\n      <td>Traffic</td>\n      <td>VEHICLE ACCIDENT -</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "[('lat', 'double'),\n ('lng', 'double'),\n ('desc', 'string'),\n ('zip', 'int'),\n ('title', 'string'),\n ('timeStamp', 'timestamp'),\n ('twp', 'string'),\n ('addr', 'string'),\n ('e', 'int'),\n ('majorTitle', 'string'),\n ('minorTitle', 'string')]"
     },
     "metadata": {}
    }
   ],
   "source": [
    "try:\n",
    "    # In case the previous query wasn't stopped\n",
    "    tq.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tq = (\n",
    "    # Create an output stream\n",
    "    flattened_stream.writeStream               \n",
    "    # Only write new rows to the output\n",
    "    .outputMode(\"append\")           \n",
    "    # Write output stream to an in-memory Spark table (a DataFrame)\n",
    "    .format(\"memory\")               \n",
    "    # The name of the output table will be the same as the name of the query\n",
    "    .queryName(\"test_query\")\n",
    "    # Submit the query to Spark and execute it\n",
    "    .start()\n",
    ")\n",
    "\n",
    "sleep(2)\n",
    "\n",
    "# When the status says \"Waiting for data to arrive\", that means the query\n",
    "# has finished its current iteration and is waiting for new messages from\n",
    "# Kafka.\n",
    "display(tq.status)\n",
    "\n",
    "memory_sink = spark.table(\"test_query\")\n",
    "\n",
    "\n",
    "# Show result table in Jupyter Notebook. Since Jupyter Notebooks have native support for showing pandas tables,\n",
    "# we convert the Spark DataFrame.\n",
    "display(memory_sink.toPandas().head(10))\n",
    "display(memory_sink.dtypes)\n",
    "\n",
    "# Stop the query\n",
    "tq.stop()"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Notice above that the ZIP contains NaNs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Our EDA (see project lab 1) showed that the only columns that contained NULL/NaN values were:\n",
    "* ZIP\n",
    "* twp (Township)\n",
    "\n",
    "These columns are of minor importance and will probably not be used, thus we will not drop any rows from our dataset.\n",
    "\n",
    "However, we notice that Township is also included in the description of a row. As such, it might be possible to fix these NULLs in township using this description field. The description field consists of values separated by a semicolon. The township the second value in this list of values. Unfortunately, if the township is missing from the entry, then the township is also not included in the description. Thus, the only way to fix these entries would be to look at the ZIP code of this entry and find entries with the same ZIP code. We could then use those entries their township to fill in the missing township. Unfortunately, there are about 266 times more ZIP codes that are missing, so we opted not to do this.\n",
    "\n",
    "We could try to fix the ZIP codes by looking at the township. However, multiple ZIP codes are possible for the same township, so we would have to narrow our search to also compare the streets. The chance that these streets appear multiple times in this dataset might be relatively small for certain entries that we are trying to fix.\n",
    "\n",
    "We decided to use the longitude and latitude in the Google Maps API to figure out what the ZIP code of a location is.\n",
    "\n",
    "**Unfortunately, we were unable to figure out how we could update our stream with the dataframe that now contains the correct ZIP codes (so longer NULL values). We understand that doing it this way, we only adjust the memory_sink pyspark dataframe, but we wanted to show that our function works and fills in ZIPs correctly. We attempted to apply this on the flattened_stream object as well, in a similar fashion as how we do it now, with `flattened_stream` instead of `memory_sink`, but alas, this would wipe our whole `flattened_stream`. The result would be an empty stream. Now, we notice that indeed the stream does not contain the adjusted ZIPs, because we only performed the changes on a local pyspark dataframe in memory, but we had no other way than to do it like this to show that we managed to fill in all ZIPs correctly. Please let us know how we can 'persist' these changes to our stream object, which we can then stream to our Kafka topic `ingest-cleaned`.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# Function to fetch township and zip from Google Maps API based on longitude and latitude of our 911 emergency\n",
    "def reverse_geocode(lat, lng, zip):\n",
    "    # Request to Google Maps.\n",
    "    try:  \n",
    "        response = requests.get('https://maps.googleapis.com/maps/api/geocode/json?latlng=' + str(lat) + ',' + str(lng) + '&key=AIzaSyBCs46NUg8q6lcAH6aIsVQwfQ4SEMIBob4')\n",
    "        \n",
    "        # Checking if the length of the zipcode is 5 characters and starts with a 1 (-> Pennsylvania)\n",
    "        for i in range(len(response.json()['results']) - 1):\n",
    "            zipcode = response.json()['results'][i]['formatted_address'].split(',')[-2].split()[1]\n",
    "            \n",
    "            if len(zipcode) == 5 and zipcode[0] == '1':\n",
    "                zipcode = zipcode\n",
    "                break\n",
    "            else:\n",
    "                zipcode = '-1'\n",
    "\n",
    "        return zipcode\n",
    "    \n",
    "    except:\n",
    "        return '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this @#!@? doesn't seem to work :D ...\n",
    "\n",
    "# UDF function which calls the reverse_geocode function and fills in the ZIP code appropriately\n",
    "\n",
    "# zip_udf = udf(lambda x: reverse_geocode(x[0], x[1]), IntegerType())\n",
    "\n",
    "# Update the zip column appropriately\n",
    "# the code below results in an empty stream, even though we've done this in a similar fashion in other cells before\n",
    "\n",
    "# flattened_stream = flattened_stream.withColumn('zip', zip_udf(struct(['lat', 'lng'])))            "
   ]
  },
  {
   "source": [
    "Let's perform our changes based on the above function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "{'message': 'Processing new data',\n 'isDataAvailable': True,\n 'isTriggerActive': True}"
     },
     "metadata": {}
    }
   ],
   "source": [
    "try:\n",
    "    # In case the previous query wasn't stopped\n",
    "    tq.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tq = (\n",
    "    # Create an output stream\n",
    "    flattened_stream.writeStream               \n",
    "    # Only write new rows to the output\n",
    "    .outputMode(\"append\")           \n",
    "    # Write output stream to an in-memory Spark table (a DataFrame)\n",
    "    .format(\"memory\")               \n",
    "    # The name of the output table will be the same as the name of the query\n",
    "    .queryName(\"test_query\")\n",
    "    # Submit the query to Spark and execute it\n",
    "    .start()\n",
    ")\n",
    "\n",
    "sleep(2)\n",
    "\n",
    "# When the status says \"Waiting for data to arrive\", that means the query\n",
    "# has finished its current iteration and is waiting for new messages from\n",
    "# Kafka.\n",
    "display(tq.status)\n",
    "memory_sink = spark.table(\"test_query\")\n",
    "\n",
    "# This changes the ZIPs correctly on the memory_sink dataframe, but how to we persist these changes to our streaming dataframe :(\n",
    "# We can't apply this to our streaming dataframe either, because this results in an empty streaming dataframe...\n",
    "# UDF function which calls the reverse_geocode function and fills in the ZIP code appropriately\n",
    "zip_udf = udf(lambda x: int(reverse_geocode(x[0], x[1], x[2])), IntegerType())\n",
    "# Update the zip column appropriately\n",
    "memory_sink = memory_sink.withColumn('zip', zip_udf(struct(['lat', 'lng', 'zip'])))\n",
    "\n",
    "# Show result table in Jupyter Notebook. Since Jupyter Notebooks have native support for showing pandas tables,\n",
    "# we convert the Spark DataFrame.\n",
    "display(memory_sink.toPandas().head(10))\n",
    "display(memory_sink.dtypes)\n",
    "\n",
    "\n",
    "# Stop the query\n",
    "tq.stop()"
   ]
  },
  {
   "source": [
    "Notice in the above table that the NaNs were replaced by ZIP codes. Unfortunately, as mentioned earlier, we do not know how we can persist these data cleaning changes to our stream, to then consequently stream it to our Kafka topic."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Write entries to ingest-cleaned Kafka topic"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Finally, we want to write the cleaned 911 entries to the `ingest-cleaned` Kafka topic. This Kafka output stream expects a dataframe, a value and an optional key column.\n",
    "\n",
    "To create the `value` column, we first create a struct from all columns in the dataframe by using the `struct` function, serialize the result to json using `to_json`, and keep only the value column using `select` and `alias`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_stream = flattened_stream.select(to_json(struct(\"*\")).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "{'message': 'Waiting for data to arrive',\n 'isDataAvailable': False,\n 'isTriggerActive': False}"
     },
     "metadata": {}
    }
   ],
   "source": [
    "try:\n",
    "    # In case the previous query wasn't stopped\n",
    "    tq.stop()\n",
    "    # Remove old checkpoint dir, other you'll get weird runtime faults\n",
    "    os.rmdir(\"checkpoints-cleanup\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Prepare df for Kafka and write to kafka\n",
    "tq = (\n",
    "    output_stream\n",
    "    .writeStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"topic\", \"ingest-cleaned\")\n",
    "    .option(\"checkpointLocation\", \"checkpoints-cleanup\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "sleep(2)\n",
    "display(tq.status)\n"
   ]
  }
 ]
}